#!/usr/bin/python

import re
import os
import sys
import json
import time
import optparse
import ConfigParser

import rrdtool
import htcondor


def parse_args():
    parser = optparse.OptionParser()
    parser.add_option("-c", "--config", help="Prodview configuration file", dest="config", default=None)
    parser.add_option("-p", "--pool", help="HTCondor pool to analyze", dest="pool")
    parser.add_option("-o", "--output", help="Top-level output dir", dest="output")
    opts, args = parser.parse_args()

    if args:
        parser.print_help()
        print >> sys.stderr, "%s takes no arguments." % sys.args[0]
        sys.exit(1)

    cp = ConfigParser.ConfigParser()
    if opts.config:
        if not os.path.exists(opts.config):
            print >> sys.stderr, "Config file %s does not exist." % opts.config
            sys.exit(1)
        cp.read(opts.config)
    elif os.path.exists("/etc/prodview.conf"):
        cp.read("/etc/prodview.conf")

    if not opts.pool and cp.has_option("htcondor", "pool"):
        opts.pool = cp.get("htcondor", "pool")
    if not opts.output and cp.has_option("totalview", "basedir"):
        opts.output = cp.get("totalview", "basedir")
    opts.analysisview = cp.get("analysisview", "basedir")
    opts.prodview = cp.get("prodview", "basedir")
    opts.analysiscrab2view = cp.get("analysiscrab2view", "basedir")

    return opts, args


def query_schedd(ad, workflows, taskInfo):
     #Query schedds to get their information and for static information
     #TODO
#    schedd = htcondor.Schedd(ad)
#    try:
#        jobs = schedd.xquery("CRAB_ReqName isnt null", ["CRAB_UserHN","CRAB_ReqName", "ClusterId", "RequestMemory", "MaxWallTimeMins", "JobStatus", "TaskType", "DESIRED_Sites", "MATCH_EXP_JOBGLIDEIN_CMSSite",  "CRAB_UserWebDir", "JobPrio", "DAG_NodesReady", "DAG_NodesUnready", "DAG_NodesTotal", "DAG_NodesFailed", "DAG_NodesDone"])
#    except Exception, e:
#        print "Failed querying", ad["Name"]
#       print e
    return


def load_pool_data(totals, sites, basedir, name):
    totals[name] = {}
    sites[name] = {}
    fname = os.path.join(basedir, 'totals.json')
    try:
        totalsSummary = json.load(open(fname))
        totals[name] = totalsSummary
    except:
        print 'Got Error loading file'
    fname = os.path.join(basedir, 'site_summary.json')
    try:
        sitesSummary = json.load(open(fname))
        sites[name] = sitesSummary
    except:
        print 'Got error loading file'
    return


def summarize(totals, sites):
    totalR = 0
    totalI = 0
    for type, type_dict in totals.items():
        if 'Running' in type_dict.keys():
            totalR += type_dict['Running']
        if 'Idle' in type_dict.keys():
            totalI += type_dict['Idle']
    totals['Summary'] = {}
    totals['Summary']['Running'] = totalR
    totals['Summary']['Idle'] = totalI
    
    sites['Summary'] = {}
    for type, type_dict in sites.items():
        if not type == 'Summary':
            for site, site_dict in type_dict.items():
                if site and site != "Summary":
                    if site not in sites['Summary'].keys():
                        sites['Summary'][site] = {"Running": 0, "MatchingIdle": 0}
                    sites['Summary'][site]['Running'] += site_dict['Running']
                    sites['Summary'][site]['MatchingIdle'] += site_dict['MatchingIdle']
                    sites['Summary'][site][type] = site_dict


def drop_obj(obj, dirname, fname):
    dirname = re.sub('[:]', '', dirname)
    fname = re.sub('[:]', '', fname)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    fname_tmp = os.path.join(dirname, fname + ".tmp")
    fname = os.path.join(dirname, fname)
    json.dump(obj, open(fname_tmp, "w"))
    os.rename(fname_tmp, fname)


def write_json(totals, sites, output):

    drop_obj(totals['Summary'], output, 'summary.json')
    drop_obj(totals, output, 'totals.json')
    sitesT = sites.get('Summary', {})
    drop_obj(sitesT, output, 'site_summary.json')
    for site, site_dict in sitesT.items():
        site_dir = os.path.join(output, site)
        drop_obj(site_dict, site_dir, "summary.json")


def write_rrds(totals, sites, output):

    running = totals['Summary']['Running']
    idle = totals['Summary']['Idle']
    fname = str(os.path.join(output, "summary.rrd"))
    if not os.path.exists(fname):
        rrdtool.create(fname,
            "--step", "180",
            "DS:Running:GAUGE:360:U:U",
            "DS:Idle:GAUGE:360:U:U",
            "RRA:AVERAGE:0.5:1:1000",
            "RRA:MIN:0.5:20:2000",
            "RRA:MAX:0.5:20:2000",
            "RRA:AVERAGE:0.5:20:2000",
        )
    rrdtool.update(fname, "N:%d:%d" % (running, idle))

    for site, site_dict in sites['Summary'].items():
        fname = str(os.path.join(output, "%s.rrd" % site))
        if not os.path.exists(fname):
            rrdtool.create(fname,
                "--step", "180",
                "DS:Running:GAUGE:360:U:U",
                "DS:MatchingIdle:GAUGE:360:U:U",
                "RRA:AVERAGE:0.5:1:1000",
                "RRA:MIN:0.5:20:2000",
                "RRA:MAX:0.5:20:2000",
                "RRA:AVERAGE:0.5:20:2000",
            )   
        rrdtool.update(fname, "N:%d:%d" % (site_dict["Running"], site_dict["MatchingIdle"]))

    #Create fake empty.rrd for sites which have no data.
    fname = str(os.path.join(output, "empty.rrd"))
    if not os.path.exists(fname):
        rrdtool.create(fname,
                "--step", "180",
                "DS:Running:GAUGE:360:U:U",
                "DS:MatchingIdle:GAUGE:360:U:U",
                "RRA:AVERAGE:0.5:1:1000",
                "RRA:MIN:0.5:20:2000",
                "RRA:MAX:0.5:20:2000",
                "RRA:AVERAGE:0.5:20:2000",
            )
        rrdtool.update(fname, "N:0:0")

def main():
    opts, args = parse_args()

    if opts.pool:
        coll = htcondor.Collector(opts.pool)
    else:
        coll = htcondor.Collector()

    #schedd_ads = coll.query(htcondor.AdTypes.Schedd, 'CMSGWMS_Type=?="crabschedd"', ['Name', 'MyAddress', 'ScheddIpAddr'])
    
    #for ad in schedd_ads:
    #    print "Querying schedd", ad['Name']
    #    query_schedd(ad, workflows, taskInfo)

    totals = {}
    sites = {}

    load_pool_data(totals, sites, opts.analysisview, "analysisview")
    load_pool_data(totals, sites, opts.prodview, "prodview")
    load_pool_data(totals, sites, opts.analysiscrab2view, "analysiscrab2view")
    #load all total jsons.
    #load sites json create final json and rrds

    summarize(totals, sites)

    if opts.output:
        write_json(totals, sites, opts.output)
        write_rrds(totals, sites, opts.output)

if __name__ == "__main__":
    main()

